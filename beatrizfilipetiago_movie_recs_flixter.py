# -*- coding: utf-8 -*-
"""BeatrizFilipeTiago_Movie_Recs_Flixter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U9JTlHfXqQMcfnSSMu2v95gZN6Lj_3mN

<h1><center>Movies Recommendation in Flixster</center></h1>

<h2><center>Data Mining II</center></h2>
<center>Beatriz Pinto    201503815, Filipe Justiça 201606339, Tiago Coelho 201604170 </center>
<p ><center>Faculdade de Ciências da Universidade do Porto<center><p>

# Introduction  
### Context and Motivation

<div style='text-align: justify;'>  
    <br>
&nbsp;&nbsp;&nbsp;Recommendation Systems have become very popular and an essential key to the success of most social media platforms, online stores and other media-services distributors. <br>
&nbsp;&nbsp;The recommendation task is one of the Web Mining applications that has been studied and developed to find patterns and learn information taking advantage of the large amounts of user data available from online usage. Companies like <i>Youtube</i>, <i>Netflix</i> or <i>Amazon</i> have been using these approaches to understand their clients and users needs and provide them recommendations will improve their engagement in the platform. Another common application  of Web Usage Mining is the discovery of user communities and profiles to have more efficient advertising. Target and recommended adds are being used more and more each day, and since publicity is transversal and applicable to almost every type of commercial businesses there is really a lot work to be done.  <br>
&nbsp;&nbsp;Dealing with such large and sparse inputs of text, metadata, images and other log information from web usage can be very challenging computationally, but once we are able to manage it the applications have a lot of potential to enhance both the user experience and the companies' reach and profit. Therefore it is mandatory to anyone working in Web Mining to understand and know how to apply the existing recommendation systems approaches. <br> 
&nbsp;&nbsp;With this work we aim to do exactly this and apply different types of recommendation algorithms that we have learned to real world data and compare their performances.
</div>

### Problem Definition and Methodology
<br>
<div style='text-align: justify;'>  
&nbsp;&nbsp;The Recommendation Problem is typically characterized by 2 objects: a set of users and set of items to be recommended to the users, and the goal is learning the function that maps the relevance and usefulness of a particular item for a user. There are two main prediction tasks that can be done depending on the context and data in our problem: 
    <br>
<ul>
<li>Item Prediction: Gives an ordered list of items which are likely to be consumed or liked by the user on the future;</li>
<li>Rating prediction: Gives a predicted rating that the user is likely to give to a certain item that he hasn't seen yet.</li>
</ul>
    <br>
&nbsp;&nbsp;In the proposed assignment we are dealing with data collected from the website <i>Flixter</i> which is a social movie platform where the users can look for movie recommendations and also share their opinions and give ratings to movies they have seen. Given this we have the standard recommendation problem setting where our task is to predict the best movie recommendations to give to a user or the rating that a user might give to a movie.
 <br>
&nbsp;&nbsp;In order to achieve this goal we start by performing an Exploratory Data Analysis where we first get to know the Datasets and informations available and then delve into the main features and behaviors captured by this data. Here we also do some Data Visualization that help us understand which attributes are important to include in our models, which time periods are statistically significant, among others. 
<br>
&nbsp;&nbsp;The following step was to build our recommendation algorithms. In the section <i>Recommendation Models</i> we begin by briefly explaining and giving examples of the reasoning behind each model's approach, followed by the experimental steps and implementation. The algorithms we have decided to work are based on the following approaches:<br>
<ul>
<li>Popularity </li>
<li>Association Rules </li>
<li>Collaborative filtering</li>
<li>Hybrid Approaches</li>
</ul>
<br>And lastly we use several evaluation metrics to perform tests and comparisons between the performances of the models' implementations which we explain and show the results in a intuitive and visual way. To do this we train our models to learn the best recommendations in a given time period and then test those recommendations based on what the users have watched in reality in the following time period. For each model we calculate the Precision, Recall, F1-score and the percentage of users that have watched at least one of our recommendations, and those measures are given by the following:
    <br>
    <br>
&nbsp;&nbsp;$Precision=\frac{number \, of \, movie \, recommendations \, the \, user \, has \, watched}{number \,of \, movies \, that \, were \, recommended}$
    <br>
    <br>
&nbsp;&nbsp;$Recall=\frac{number \, of \, movie \, recommendations \, the \, user \, has \, watched}{number \,of \, movies \, that \, the \, user \, has \, watched \, in \, the \, same \, period}$
    <br>
    <br>
&nbsp;&nbsp;$F1\, score=\frac{2*Recall*Precision}{Recall+Precision}$
    <br>
    <br>
&nbsp;&nbsp;It is important to note that the <i>number of movies that the user has watched in the same period</i> is not the total number of movies the user has watched, but the number of movies he has watched within the subset of possible recommendations which we considered to be the 50 most popular movies of that time period. <br>
    <br>
&nbsp;&nbsp;
For each type of approach we test the models for different numbers of recommendations that are given to the user, and then try to choose the best number based on the performance measures above. Here it is important to understand that according to the way the recall is calculated, the more recommendations we give, the higher will be the recall. Therefore there should be a compromise between this and the number of recommendations given, because it would not make sense to suggest too many movies to the user. After having the selected number of recommendation for each model we consecutively train and test all the models trough a sliding window to get the final results. 
</div>

# Exploratory Data Analysis
### Handling the Datasets : Pre-processing
<br>
<div style='text-align: justify;'>  
&nbsp;&nbsp;We start we the given zip files which are composed of 4 datasets: one with the list of ratings (with 4 columns for the id       of the user -<i>userid</i>- and the movie -<i>movieid</i>- in the rating, the score -<i>rating</i>- and the date and time -<i>date</i>-) a metadata file with the user profile information (with 7 attributes: <i>userid</i>, <i>gender</i>, <i>location</i>, <i>memberfor</i>, <i>lastlogin</i>, <i>profileview</i> and <i>age</i>). and the last 2 files are actually the same and consist of the movie metadata (associating the <i>movieid</i> to the <i>name</i> of the movie). 
    <br>
&nbsp;&nbsp;Since the previous datasets were text files, we have decided to convert them to the <i>csv</i> format for an easier pre-processing phase. This type of file is better since we can easily import the data to our environment using the <i>pandas</i> package , which handles separations and data manipulation very well, and quickly create our desired dataframe. These file contain exactly the same information and attributes and the text files, we have just converted them and simplified the names. 
    <br>
&nbsp;&nbsp;So the first thing to do is import the necessary packages (<i>numpy</i>, <i>pandas</i>, <i>plotly</i> and <i>datetime</i>) and our ratings, movies and users files. While doing this we applied the <i>pandas</i> built-in methods to handle the different types of delimiters, separations and encoding of the files and also to convert the format of the <i>date</i> and <i>memberfor</i> entries to the correct python datetime format for easier access in future manipulations. So now we are ready to check our dataframes and do any necessary pre-processing taks. 
    <br>
&nbsp;&nbsp;We start by visualizing our 3 dataframes and their main characteristics by calling the routines <i>head()</i> and <i>info()</i>. We first look at the <i>ratings</i> table and check that it has 4 columns and 8196078 rows. The data types are ok, but for consistency we have decided to change the data types of <i>userid</i> and <i>movieid</i> values to <i>int</i> (previously types <i>object</i> and <i>float</i>, respectively). But before doing this we first checked any missing or repeated values. For this dataframe only the last row had any missing data, which we therefore have removed, and there were no repeated entries.
</div>

Please run the whole script to have the recomendation generator working. Beside that connect to yout google drive account and save the files that we've sent to you with the names users.csv, movies.csv and ratings.csv
"""

from google.colab import drive
drive.mount('/content/drive')

pip install chart_studio

pip install mlxtend

pip install scikit-surprise

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# 
# from __future__ import division
# from datetime import datetime, timedelta
# import pandas as pd
# import numpy as np
# 
# 
# import chart_studio.plotly as py
# import plotly.offline as pyoff
# import plotly.graph_objs as go
# from plotly.subplots import make_subplots
# 
# users=pd.read_csv("/content/drive/My Drive/users.csv",delimiter=";")
# movies=pd.read_csv("/content/drive/My Drive/movies.csv",delimiter=";",encoding = "ISO-8859-1")
# ratings=pd.read_csv("/content/drive/My Drive/ratings.csv",sep='\t', lineterminator='\r',encoding = "UTF-16 LE")
# ratings['date']= pd.to_datetime(ratings['date']) 
# users['memberfor']= pd.to_datetime(users['memberfor'],format='%d/%m/%Y %H:%M')

ratings.head()

#print(ratings.info())
#print(ratings.isnull().sum())
#print(ratings.iloc[-1])
ratings.drop([8196077],inplace=True)
#print(ratings.isnull().sum())
ratings.isnull().sum()
ratings['movieid']=pd.to_numeric(ratings['movieid'],downcast='integer')
ratings['userid']=pd.to_numeric(ratings['userid'],downcast='integer')
print(ratings.info())
print('duplicates: ',ratings.duplicated().sum())

"""<div style='text-align: justify;'>  
&nbsp;&nbsp;Next we look at the <i>users</i> table and again, checked the data types, missing and repeated values. We left the data types as  they were and there we no repeated values. Nevertheless there is a significant number of missing values in the <i>age</i> column (more than 25% of the entries), and also in the <i>gender</i> and <i>lastlogin</i> information (less than 7%). This can make it more demanding to find similarities and communities between users based on other things but their ratings, and complicates the creation of user profiles.
</div>
"""

#print(users.head())
print(users.info())
#print(users.isnull().sum())
print('duplicates: ',users.duplicated().sum())

"""<div style='text-align: justify;'>  
&nbsp;&nbsp;Lastly we have the <i>movies</i> dataframe, for which the same process has been repeated. With only 2 columns, this table did not have any duplicates or missing values, so no modifications were needed. After this making sure the columns had the same names in the 3 dataframes, we are ready to explore and get a deeper analysis of the data.
</div>
"""

#print(movies.head())
print(movies.info())
#print(movies.isnull().sum())
print('duplicates: ',movies.duplicated().sum())

"""### Data Visualization
<br>
<div style='text-align: justify;'>  
&nbsp;&nbsp;In order to understand better our set of users we start by characterizing the population in a histogram for the age. The result is in the following image and tells us that most users are between 18 and 30 years old. Another thing to take in consideration is that, even though there 1976 users with ages between 107 and 109, there are almost no users older than 72 and younger than 107. We think this values might be due to a systematic error and are probably outliers, but still decided to consider their ratings.
</div>
"""

plot_data = [
    go.Histogram(
        x=users['age'],
    )
]

plot_layout = go.Layout(
        title='Age Distriution', 
        xaxis_title="Age",
        yaxis_title="Number of Users"
        
    )

fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

"""<div style='text-align: justify;'>  
&nbsp;&nbsp;Next we studied the daily number of new registered users and plot the respective histogram using the <i>memberfor</i> date values. As we can see, there are only new registrations in the first 3 days of each month which is unsual, but can be due some to the subscription or the way the system is programmed to update the user data. With this is mind we thought it would be relevant to check if there are ratings made by a user previously to their registered date of becoming a member. So when this happens we change the <i>memberfor</i> field to the <i>date</i> or the user's earliest rating. Another thing to be noticed is that there are entries that belong to the year of 1900 which is of course impossible and therefore those users will not be considered.
</div>
"""

tx_data=users
tx_data['memberforYearMonthDay'] = tx_data['memberfor'].map(lambda date: date )


newUsersmonth = tx_data.groupby(['memberforYearMonthDay'])['userid'].nunique().reset_index()
plot_data = [
    go.Bar(
        x=newUsersmonth['memberforYearMonthDay'],
        y=newUsersmonth['userid'],
    )
]

plot_layout = go.Layout(
        xaxis={"type": "category"},
        yaxis_title="Number of Users",
        title='New users'
    )

fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

"""<div style='text-align: justify;'>  
&nbsp;&nbsp;And then we look at the volume of ratings made every month and checked that the first semester of 2007 was the period when more ratings were added to the site. This information will be important for later when making predictions and testing their performance for the several recommendation systems.
</div>
"""

tx_data=ratings
tx_data['date'] = tx_data['date'].map(lambda date: 100*date.year + date.month )


newcomentsmonth = tx_data.groupby(['date'])['userid'].count().reset_index()
plot_data = [
    go.Bar(
        x=newcomentsmonth['date'],
        y=newcomentsmonth['userid'],
    )
]

plot_layout = go.Layout(
        xaxis={"type": "category"},
        yaxis_title="Number of ratings",
        title='New monthly ratings'
    )

fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

tx_data=ratings
tx_data=pd.merge(tx_data,movies, on="movieid", how="left")
tx_data.head()
views = tx_data.groupby(['movieid'])['userid'].count().reset_index()
views=views.sort_values(by='userid', ascending=False)
views=views.rename(columns={"userid": "Views"})
views=pd.merge(views,movies, on="movieid", how="right")
views.head();

"""<div style='text-align: justify;'>  
&nbsp;&nbsp;We now look for the top 10 most popular movies of all time registered in the data. To do this, a new dataframe is created by joining the entries in <i>ratings</i> by the value of <i>movieid</i>. First we show the top 10 movies with more ratings, meaning higher number of views, in the following histogram. And then compare with a top 10 calculated with a weighted mean of the number of views and the mean rating score given by the users.
    <br>
&nbsp;&nbsp;The list of movies is not the same but the top 4 most viewed were also the best rated in this group, so there is probably a correlation with the number of views and the score of the movie.
<div>
"""

plot_data = [
    go.Bar(
        x=views['movieid'][0:10],
        y=views['Views'][0:10],
        text=views["moviename"][0:10] ,textposition='inside', marker_color='lightsalmon'
    )
]

plot_layout = go.Layout(
        xaxis={"type": "category"},
        title='Top 10 most viewed movies',
        xaxis_title="movieid",
        yaxis_title="Views"
        
    )

fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)
movierank = tx_data.groupby(['movieid'])['rating'].mean().reset_index()
movierank = pd.merge(movierank,views, on="movieid", how="right")
movierank["FinalRating"]=(movierank["rating"]/5*0.7+0.3*movierank["Views"]/np.max(movierank["Views"]))*5
movierank = movierank.sort_values(by='FinalRating', ascending=False)
movierank.head();
plot_data = [
    go.Bar(
        x=movierank['movieid'][0:10],
        y=movierank['FinalRating'][0:10],
        text=movierank["moviename"][0:10] ,textposition='inside', marker_color='lightsalmon'
    )
]

plot_layout = go.Layout(
        xaxis={"type": "category"},
        title='Top 10 movies with best rated and most views',
        xaxis_title="movieid",
        yaxis_title="Rating"
        
    )

fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

plot_data = [
    go.Scatter(
        x=movierank.query("Views>100")['Views'],
        y=movierank.query("Views>100")['rating'],
        mode='markers'
    )
]

plot_layout = go.Layout(
        title='Rating score vs. number of views',
        xaxis_title="views",
        yaxis_title="rating score"
        
    )

fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)
plot_data = [
    go.Scatter(
        x=movierank.query("Views>100")['Views'],
        y=movierank.query("Views>100")['FinalRating'],
        mode='markers',
        hovertext=movierank.query("Views>100")["moviename"]
    )
]

plot_layout = go.Layout(
        title="Final Rating (weighted averagem) vs. number of views",
        xaxis_title="views",
        yaxis_title="views"
        
    )

fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

def most_viewed_age(age1,age2):
  tx_data=ratings
  tx_data=pd.merge(tx_data,movies, on="movieid", how="left")
  tx_data=pd.merge(tx_data,users[["userid","age"]], on="userid", how="left")
  tx_age=tx_data.query(str(age1)+"<=age<="+str(age2))
  views = tx_age.groupby(['movieid'])['userid'].count().reset_index()
  views=views.sort_values(by='userid', ascending=False)
  views=views.rename(columns={"userid": "Views"})
  views=pd.merge(views,movies, on="movieid", how="right")
  return views

#most_viewed_age(20,20)

specs = [[{'type':'domain'}, {'type':'domain'}], [{'type':'domain'}, {'type':'domain'}]]
subplot_titles=['Age 10-20', 'Age 20-30','Age 30-40','Age 40-50']
fig = make_subplots(rows=2, cols=2, specs=specs, subplot_titles=subplot_titles)
fig.add_trace(go.Pie(labels=most_viewed_age(10,20)["moviename"][0:10], 
                     values=most_viewed_age(10,20)["Views"][0:10]), 1, 1)
fig.add_trace(go.Pie(labels=most_viewed_age(20,30)["moviename"][0:10], 
                     values=most_viewed_age(20,30)["Views"][0:10]), 2, 1)
fig.add_trace(go.Pie(labels=most_viewed_age(30,40)["moviename"][0:10], 
                     values=most_viewed_age(30,40)["Views"][0:10]), 1, 2)
fig.add_trace(go.Pie(labels=most_viewed_age(40,50)["moviename"][0:10], 
                     values=most_viewed_age(40,50)["Views"][0:10]), 2, 2)

# Tune layout and hover info
fig.update_traces(hoverinfo='label+percent+name', textinfo='label', textposition='inside')
fig.update(layout_title_text='Mais vistos por idade',
           layout_showlegend=False)

fig = go.Figure(fig)
fig.show()

users=pd.read_csv("/content/drive/My Drive/users.csv",delimiter=";")
movies=pd.read_csv("/content/drive/My Drive/movies.csv",delimiter=";",encoding = "ISO-8859-1")
ratings=pd.read_csv("/content/drive/My Drive/ratings.csv",sep='\t', lineterminator='\r',encoding = "UTF-16 LE")
ratings['date']= pd.to_datetime(ratings['date']) 
users['memberfor']= pd.to_datetime(users['memberfor'],format='%d/%m/%Y %H:%M') 
ratings.drop([8196077],inplace=True)
#print(ratings.isnull().sum())
ratings.isnull().sum()
ratings['movieid']=pd.to_numeric(ratings['movieid'],downcast='integer')
ratings['userid']=pd.to_numeric(ratings['userid'],downcast='integer')

###function that filters the movieids and names to have an ordered list of the most rated/seen movies in a time
#window starting in date 1 and ending in date 2
def pop_by_data(date1,date2):
    d1=pd.to_datetime(date1, format='%d/%m/%Y')
    d2=pd.to_datetime(date2, format='%d/%m/%Y')
    mask = (ratings['date'] >= d1) & (ratings['date'] <= d2)
    tx_data=ratings.loc[mask]
    tx_data=pd.merge(tx_data,movies, on="movieid", how="left")
    views = tx_data.groupby(['movieid'])['userid'].count().reset_index()
    views=views.sort_values(by='userid', ascending=False)
    views=views.rename(columns={"userid": "Views"})
    views=pd.merge(views,movies, on="movieid", how="left")
    return views
#%timeit pop_by_data('01/05/2007','01/06/2007')
#pop_by_data('01/05/2007','01/06/2007')



###this function returns a dataframe that tell us if a movie was watched by
'''
def pivot(date1,date2):
  d2=pd.to_datetime(date2, format='%d/%m/%Y')
  tx_user = ratings.loc[ratings['date'] <= d2]
  tx_user=tx_user[tx_user.movieid.isin(pop_by_data(date1,date2)["movieid"][0:10])]
  index=pd.pivot_table(tx_user, index='userid', columns='movieid', values='rating')
  return index
#%timeit pivot('01/05/2007','01/06/2007')
pivot('01/05/2007','01/06/2007')
'''

def revert_pivot(date1,date2):
  d2=pd.to_datetime(date2, format='%d/%m/%Y')
  tx_user = ratings.loc[ratings['date'] <= d2]
  tx_user=tx_user[tx_user.movieid.isin(pop_by_data(date1,date2)["movieid"][0:50])]
  index=pd.pivot_table(tx_user, index='movieid', columns='userid', values='rating')
  new_order=pop_by_data(date1,date2)["movieid"][0:50].tolist()
  index=index.reindex(new_order)
  return index
#rp=revert_pivot('01/05/2007','01/06/2007')
#rp.columns


def r_lista(userid,rp,n):
  return rp[np.isnan(rp[userid])].index.tolist()[0:n]

def table(date1,date2,N):
  rp=revert_pivot(date1,date2)
  users=rp.columns.to_numpy()
  vfunc = np.vectorize(r_lista, excluded=["rp","n"], otypes=[list])
  return pd.DataFrame(data={"userid":users, "recommended_p_movieids":vfunc(users,rp=rp,n=N)})

"""<div style='text-align: justify;'>  
&nbsp;&nbsp;
After having implemented this steps we need to check how relevant and accurate our recommendations would have been for the users. To have this information, we look at the data, in particular the ratings, made in the following period (in this case 1 month again) and compare the list of movies watched by the user head to head with the list of suggested movies we got for that user. <br>
&nbsp;&nbsp;The following table shows an example of the results of the model when we give 5 recommendations to the user. For each user we have the list of 5 movies we had recommended (based in the ones he had already seen in the past and the most popular movies in the previous month), the list of movies he has in fact watch in the following month, and the number of successful recommendations (0-5: number of movies from the list of recommendations that he has seen).
    <div>
"""

def revert_pivot_after(date1,date2,date3):
  d2=pd.to_datetime(date2, format='%d/%m/%Y')
  d3=pd.to_datetime(date3, format='%d/%m/%Y')
  tx_user = ratings.loc[(ratings['date'] <= d3) & (ratings['date'] >= d2)]
  tx_user=tx_user[tx_user.movieid.isin(pop_by_data(date1,date2)["movieid"][0:50])]
  index=pd.pivot_table(tx_user, index='movieid', columns='userid', values='rating')
  new_order=pop_by_data(date1,date2)["movieid"][0:50].tolist()
  index=index.reindex(new_order)
  return index


def r_lista_after(userid,rp):
  return rp[np.isnan(rp[userid])!=True].index.tolist()
def table_after(date1,date2,date3):
  rp=revert_pivot_after(date1,date2,date3)
  users=rp.columns.to_numpy()
  vfunc = np.vectorize(r_lista_after, excluded=["rp"], otypes=[list])
  return pd.DataFrame(data={"userid":users, "seen_foll_month":vfunc(users,rp=rp)})
verif=table_after('01/05/2007','01/06/2007','01/07/2007')


def is_true(userid,r):
  u=pd.DataFrame(data={"movies": r.loc[userid,"seen_foll_month"]+r.loc[userid,"recommended_p_movieids"]})
  return len(u[u.duplicated()])
def label(r):
  users=r.dropna(subset=['seen_foll_month'])["userid"].to_numpy()

  vfunc = np.vectorize(is_true, excluded=["r"], otypes=[int])
  r_index=r.set_index('userid')
  num=vfunc(users,r=r_index)
  label=pd.DataFrame(data={"userid":users, "n_success_recommendations":num})
  r=pd.merge(r,label,on='userid',how='left')
  r["n_success_recommendations"].fillna(0,inplace=True)
  return r



def K_recomendacoes_populares(date1,date2,date3,N):
  recomendas=table(date1,date2,N)
  verif=table_after(date1,date2,date3)
  recomendas=pd.merge(recomendas,verif,on='userid',how='left')
  recomendas=label(recomendas)
  recomendas["seen_foll_month"].fillna(0,inplace=True)
  recomendas["recomlen"]=recomendas["recommended_p_movieids"].apply(len)
  recomendas["nºvistas"]=recomendas["seen_foll_month"].apply(lambda x: len(x) if x!=0 else 0)
  recomendas["recall"]=recomendas["n_success_recommendations"]/recomendas["nºvistas"]
  recomendas["recall"].fillna(0,inplace=True)
  return recomendas 

#r=K_recomendacoes_populares('01/05/2007','01/06/2007','01/07/2007',5)
#r.iloc[595:599]

"""<div style='text-align: justify;'>  
&nbsp;&nbsp;Since we are using training data, our recommendations did not really affect the choices of the users. As the results illustrate, there are many users that have actually not watch any movie in the following month, and when giving them recommendations we are not distinguishing between a user that is going to stay active in the site and one that is not. <br>
&nbsp;&nbsp;For this reason, when calculating precision, recall and other performance measures, we only consider users that have watched at least one movie in the test time period (the following month). Nevertheless in a real world context application, it would be important to make recommendations to users that are likely to become inactive, since that would probably remind them to keep watching movies by suggesting things that they are interested in. <br>
&nbsp;&nbsp;In the following table we have gathered the performance measures Precision, Recall, F1-score and percentage of users that have seen at least one of the recommendations for 3, 5, 10 or 20 recommendations. As was previously mentioned, there is an expected proportional relationship between the number of recommendations and the recall. So although we have a higher score with 20 recommendations, we won't be choosing this as the best model, not only because they are too many, but also because they give a very low accuracy. After considering the advantages of each number of recommendations, we have decided to choose 5 as the best model.
    
<div>
"""

precision=[]
percentage=[]
n=[]
recall=[]
f_score=[]
for i in [3,5,10,20]:
  n.append(i)
  r=K_recomendacoes_populares('01/05/2007','01/06/2007','01/07/2007',i)
  precision.append(str(np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100)[0:4]+"%")
  recall.append(str(np.mean(r.query("seen_foll_month != 0")["recall"])*100)[0:4]+"%")
  p=np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100
  R=np.mean(r.query("seen_foll_month != 0")["recall"])*100
  f_score.append(str(2*p*R/(R+p))[0:4]+"%")
  percentage.append(str(r.query("n_success_recommendations > 0")["n_success_recommendations"].count()/r.query("seen_foll_month != 0")["seen_foll_month"].count()*100)[0:4]+"%")
measures = pd.DataFrame(data={"Precision":precision,"Recall":recall,"F1":f_score,"% of users that watched 1 or more recommendations":percentage, "n of recommendations":n})
measures = measures.set_index('n of recommendations')
measures

"""Recomendations based on popularity and age



"""

def revert_pivot_after(date1,date2,date3):
  d2=pd.to_datetime(date2, format='%d/%m/%Y')
  d3=pd.to_datetime(date3, format='%d/%m/%Y')
  tx_user = ratings.loc[(ratings['date'] <= d3) & (ratings['date'] >= d2)]
  tx_user=tx_user[tx_user.movieid.isin(pop_by_data(date1,date2)["movieid"][0:50])]
  index=pd.pivot_table(tx_user, index='movieid', columns='userid', values='rating')
  new_order=pop_by_data(date1,date2)["movieid"][0:50].tolist()
  index=index.reindex(new_order)
  return index


def r_lista_after(userid,rp):
  return rp[np.isnan(rp[userid])!=True].index.tolist()
def table_after(date1,date2,date3):
  rp=revert_pivot_after(date1,date2,date3)
  users=rp.columns.to_numpy()
  vfunc = np.vectorize(r_lista_after, excluded=["rp"], otypes=[list])
  return pd.DataFrame(data={"userid":users, "seen_foll_month":vfunc(users,rp=rp)})
verif=table_after('01/05/2007','01/06/2007','01/07/2007')


def is_true(userid,r):
  u=pd.DataFrame(data={"movies": r.loc[userid,"seen_foll_month"]+r.loc[userid,"recommended_p_movieids"]})
  return len(u[u.duplicated()])
def label(r):
  users=r.dropna(subset=['seen_foll_month'])["userid"].to_numpy()

  vfunc = np.vectorize(is_true, excluded=["r"], otypes=[int])
  r_index=r.set_index('userid')
  num=vfunc(users,r=r_index)
  label=pd.DataFrame(data={"userid":users, "n_success_recommendations":num})
  r=pd.merge(r,label,on='userid',how='left')
  r["n_success_recommendations"].fillna(0,inplace=True)
  return r



def K_recomendacoes_populares(date1,date2,date3,N):
  recomendas=table(date1,date2,N)
  verif=table_after(date1,date2,date3)
  recomendas=pd.merge(recomendas,verif,on='userid',how='left')
  recomendas=label(recomendas)
  recomendas["seen_foll_month"].fillna(0,inplace=True)
  recomendas["recomlen"]=recomendas["recommended_p_movieids"].apply(len)
  recomendas["nºvistas"]=recomendas["seen_foll_month"].apply(lambda x: len(x) if x!=0 else 0)
  recomendas["recall"]=recomendas["n_success_recommendations"]/recomendas["nºvistas"]
  recomendas["recall"].fillna(0,inplace=True)
  return recomendas 


def pop_by_data_age(date1,date2,age1,age2):
    d1=pd.to_datetime(date1, format='%d/%m/%Y')
    d2=pd.to_datetime(date2, format='%d/%m/%Y')
    mask = (ratings['date'] >= d1) & (ratings['date'] <= d2)
    tx_data=ratings.loc[mask]
    tx_data=pd.merge(tx_data,movies, on="movieid", how="left")
    tx_data=pd.merge(tx_data,users, on="userid", how="left")
    mask2 = (tx_data["age"] >= age1) & (tx_data["age"] < age2)
    tx_data = tx_data.loc[mask2]
    views = tx_data.groupby(['movieid'])['userid'].count().reset_index()
    views=views.sort_values(by='userid', ascending=False)
    views=views.rename(columns={"userid": "Views"})
    views=pd.merge(views,movies, on="movieid", how="left")
    return views


def revert_pivot_age(date1,date2,age1,age2):
    d2=pd.to_datetime(date2, format='%d/%m/%Y')
    tx_user = ratings.loc[ratings['date'] <= d2]
    tx_user = pd.merge(tx_user,users, on="userid", how="left")
    mask2 = (tx_user["age"] >= age1) & (tx_user["age"] < age2)
    tx_user = tx_user.loc[mask2]
    tx_user=tx_user[tx_user.movieid.isin(pop_by_data_age(date1,date2,age1,age2)["movieid"][0:50])]
    index=pd.pivot_table(tx_user, index='movieid', columns='userid', values='rating')
    new_order=pop_by_data_age(date1,date2,age1,age2)["movieid"][0:50].tolist()
    index=index.reindex(new_order)
    return index

def table_age(date1,date2,N,age1,age2):
    rp=revert_pivot_age(date1,date2,age1,age2)
    users=rp.columns.to_numpy()
    vfunc = np.vectorize(r_lista, excluded=["rp","n"], otypes=[list])
    return pd.DataFrame(data={"userid":users, "recommended_p_movieids":vfunc(users,rp=rp,n=N)})

def K_recomendacoes_populares_age(date1,date2,date3,N):
    recomendas=table_age(date1,date2,N,5,15)
    for i in [[15,25],[25,35],[35,45],[45,55],[55,65]]:
        recomendas= recomendas.append(table_age(date1,date2,N,i[0],i[1]), ignore_index=True)
  
    recomendas.drop_duplicates(subset ="userid", keep = "first", inplace = True)
    verif=table_after(date1,date2,date3)
    recomendas=pd.merge(recomendas,verif,on='userid',how='left')
    recomendas=label(recomendas)
    recomendas["seen_foll_month"].fillna(0,inplace=True)
    recomendas["nºvistas"]=recomendas["seen_foll_month"].apply(lambda x: len(x) if x!=0 else 0)
    recomendas["recall"]=recomendas["n_success_recommendations"]/recomendas["nºvistas"]
    recomendas["recall"].fillna(0,inplace=True)
    return recomendas 

precision=[]
percentage=[]
n=[]
recall=[]
f_score=[]
for i in [3,5,10,20]:
    n.append(i)
    r=K_recomendacoes_populares_age('01/05/2007','01/06/2007','01/07/2007',i)
    precision.append(str(np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100)[0:4]+"%")
    recall.append(str(np.mean(r.query("seen_foll_month != 0")["recall"])*100)[0:4]+"%")
    p=np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100
    R=np.mean(r.query("seen_foll_month != 0")["recall"])*100
    f_score.append(str(2*p*R/(R+p))[0:4]+"%")
    percentage.append(str(r.query("n_success_recommendations > 0")["n_success_recommendations"].count()/r.query("seen_foll_month != 0")["n_success_recommendations"].count()*100)[0:4]+"%")

    

pop_by_data_age('01/05/2007','01/06/2007',30,50)
recomendas=table_age('01/05/2007','01/06/2007',10,20,30)   
revert_pivot_age('01/05/2007','01/06/2007',30,50)
measures = pd.DataFrame(data={"Precision":precision,"Recall":recall,"F1 score":f_score,"% of users that watched 1 or more recommendations":percentage, "n of recommendations":n})
measures = measures.set_index('n of recommendations')
print("Table of performance measures for popularity model considering age groups")
measures

"""### Association Rules
<br>
<div style='text-align: justify;'>  
&nbsp;&nbsp;Association rules were originally applied in Market Basket Analysis with the purpose of finding unexpected patterns in the way consumers buy items. 
  <br>
&nbsp;&nbsp;A transaction is the name given to a set of items being bought by a consumer. With the information of many transactions, performed by many users on several items, we aim to find the most frequent itemsets, meaning a group of items which are usually bought together. Based on this item sets we are able to build association rules that with a certain degree of confidence tells us that a particular itemset $Y$ is likely to be bought if the client buys $X$. This is $X \to Y$ where no items in $Y$ belong to $X$ and vice-versa ($X \cap Y  = \emptyset)$. <br>
    <br>
&nbsp;&nbsp;In practice, a association rule would tell us for instance that is a costumer goes to the store to buy wine and cheese, there is a high chance of him buying olives as well. So knowing this could help the store to display olives close to the wine or cheese sections.
  <br>
&nbsp;&nbsp;The most common algorithm for generating the best rules in our dataset is <i>Apriori</i> which implements 2 main steps:
     <ul>
<li> Identification of frequent itemsets </li>
<li> Generation of rules</li>
</ul>
   <br>
&nbsp;&nbsp;The <b>identification of frequent itemsets</b> in the apriori algorithm is done in levels, from the smaller size itemsets to the larger, and with a generate-and-test strategy where at each iteration the new frequent itemsets candidates of size $k$ are generated based on the previously found frequent itemsets of $k-1$ size and prune using a minimum support. The support-based pruning eliminates itemsets with a support lower than the chosen minimum support. The support of an item is just the number of transactions in the dataset where the item appears, and the support of an itemset is the union of the transactions where its items appear: $sup(X \to Y) = sup(X \cup Y)$. <br>
     <br>
&nbsp;&nbsp;The <b>generation rules</b> is done as following: first we compute all non-empty subsets $s$ of each frequent itemset $I$ and for each subset we calculate the confidence of the rule $(I-s) \to s$, where the confidence is a measure of the strength of the rule and is given by the percentage of transactions that having the antecedents, also have the descendants $conf(X \to Y) = \frac{sup(X \cup Y)}{sup(X)}$. Then we just prune the rules by eliminating the ones with confidence lower than the minimum. 
  </div>
  
#### Implementation

<div style='text-align: justify;'>  <br>
&nbsp;&nbsp;In our case of study we do not have items being bought by consumers but movies being watched by users. This is from the beginning a limitation because the patterns of the user's choices of movies should not be as strong as when people buy 2 items together in one trip to the supermarket. The user does not watch more than one movie at the same time, but we can try to guess which movies a user is likely to watch and like based on what other users that have seen similar stuff have watched. Given this, our task is to build a model that consists in a list of rules that best describe the patterns in choices of the users in the dataset.
     <br>
&nbsp;&nbsp;In order to do this we start by organizing our data in the shape of transactions where each transaction is the set of movies seen by a user until a certain point in time. In the following table we have an example of sets of movies seen by users between May and June of 2007. 
</div>
"""

users=pd.read_csv("/content/drive/My Drive/users.csv",delimiter=";")
movies=pd.read_csv("/content/drive/My Drive/movies.csv",delimiter=";",encoding = "ISO-8859-1")
ratings=pd.read_csv("/content/drive/My Drive/ratings.csv",sep='\t', lineterminator='\r',encoding = "UTF-16 LE")
ratings['date']= pd.to_datetime(ratings['date']) 
users['memberfor']= pd.to_datetime(users['memberfor'],format='%d/%m/%Y %H:%M') 
ratings.drop([8196077],inplace=True)

ratings.isnull().sum()
ratings['movieid']=pd.to_numeric(ratings['movieid'],downcast='integer')
ratings['userid']=pd.to_numeric(ratings['userid'],downcast='integer')


from mlxtend.preprocessing import TransactionEncoder
def revert_pivot_v(date1,date2):
  d1=pd.to_datetime(date1, format='%d/%m/%Y')
  d2=pd.to_datetime(date2, format='%d/%m/%Y')
  mask = (ratings['date'] >= d1) & (ratings['date'] <= d2)
  tx_user=ratings.loc[mask]
  min_movie_ratings = 0
  filter_movies = tx_user['movieid'].value_counts() > min_movie_ratings
  filter_movies = pop_by_data(date1, date2)["movieid"][0:50]

  min_user_ratings = 0
  filter_users = tx_user['userid'].value_counts() > min_user_ratings
  filter_users = filter_users[filter_users].index.tolist()
  ratings_new = tx_user[(tx_user['movieid'].isin(filter_movies)) & (tx_user['userid'].isin(filter_users))]
  index=pd.pivot_table(ratings_new, index='movieid', columns='userid', values='rating')
  return index

def viram(userid,rp):
  return rp[np.isnan(rp[userid])!=True].index.tolist()

def table_viram(date1,date2):
  rp=revert_pivot_v(date1,date2)
  users=rp.columns.to_numpy()
  vfunc = np.vectorize(viram, excluded=["rp"], otypes=[list])
  return pd.DataFrame(data={"userid":users, "seen":vfunc(users,rp=rp)})

trans=table_viram('01/05/2007','01/06/2007')
dataset=trans["seen"].tolist()
te = TransactionEncoder()
te_ary = te.fit(dataset).transform(dataset)
df = pd.DataFrame(te_ary, columns=te.columns_)

trans.iloc[:3]

print(len(trans))

"""<br>
<div style='text-align: justify;'>  
To generate our itemsets we filter the movie ratings from users that have seen at least one movie of the 50 most popular movies in the last month and use the <i></i> from the package <i></i>. Following this we can use the methods in the apriori algorithm to find the most frequent itemsets and then generate the rules. In the next table we have a sample of some frequent itemsets, their length and support for a pruning with minimum support equal to 0.03. Generally we start with a small support of around $minsup=\frac{10}{number \, of \, transactions}$, which in our case would give approximately 0.0015, but our machines took too much time to compute rules with such small support, so we had to make this number bigger to have fewer itemsets to work with.
<div>
"""

from mlxtend.frequent_patterns import apriori
frequent_itemsets = apriori(df, min_support=0.03, use_colnames=True)
frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
frequent_itemsets
frequent_itemsets=frequent_itemsets.sort_values(by="length",ascending=False)
frequent_itemsets.head()

"""<div style='text-align: justify;'>  
    Then we use the routine <i>association_rules</i> from the same package with a confidence threshold of 0.4. Here we ca see some of the best rules generated. For instance, in the first row we have that who has seen the 3 movies which are the antecedents of the rule is likely to watch the movie with id 56916  (the consequent) with a confidence of 91.5%. In total the algorithm found 3669 rules for around 7000 users.
<div>
"""

from mlxtend.frequent_patterns import association_rules
rules=association_rules(frequent_itemsets, metric="confidence", min_threshold=0.4)
rules=rules.sort_values(by=["confidence","support","lift"],ascending=[False,False,False])

rules.head()

"""<br>
<div style='text-align: justify;'>  
Now to have the final model we just have to choose rules for each user according to the movies he has watched in the past. In the following table we have 5 of the 24 rules that match the user with userid number 84 and their confidence levels. And according to the number or rules we wish to give to each user we recommend the movies that are consequents of those rules (which are the rules with higher confidence for that user).
<div>
"""

def viram2(userid,rp):
  return set(rp[np.isnan(rp[userid])!=True].index.tolist())

def viram_same(date1,date2):
  p=revert_pivot(date1,date2)
  users=p.columns.to_numpy()
  vfunc = np.vectorize(viram2, excluded=["rp"], otypes=[set])
  return pd.DataFrame(data={"userid":users, "viu":vfunc(users,rp=p)})

rec_a_rules = viram_same('01/05/2007','01/06/2007')
rec_a_rules=rec_a_rules.set_index('userid')
rules[((rules['antecedents'] >= rec_a_rules.loc[84,'viu']) | (rules['antecedents'] <= rec_a_rules.loc[84,'viu'])) & ((rules["consequents"]-rec_a_rules.loc[84,'viu'])==rules["consequents"])].groupby(["consequents"]).max()

def conf(userid,rules,rec_a_rules,k):
  re=pd.DataFrame(data=set(rules[( ( rules['antecedents'] >= rec_a_rules.loc[userid,'viu'] ) | ( rules['antecedents'] <= rec_a_rules.loc[userid,'viu'] ) ) & ( (rules["consequents"]-rec_a_rules.loc[userid,'viu'])==rules["consequents"] )].groupby(["consequents"]).max().index.to_list()[0:k]))
  n=len(re.columns)
  if (n!=0):
    data=re[0].to_list()
    for i in range(1,n):
      data+=re[i].to_list()
    re2=pd.DataFrame(data=data)
    re2=re2.drop_duplicates(keep='first')
    re2=re2.dropna()
    return re2[0].to_list()
  else:
    return []

def rec_rules(rules,rec_a_rules,N):
  users=rec_a_rules.index.to_numpy()
  vfunc = np.vectorize(conf, excluded=["rules","rec_a_rules","k"], otypes=[list])
  return pd.DataFrame(data={"userid":users, "recomendas":vfunc(users,rules=rules,rec_a_rules=rec_a_rules,k=N)})

def r_lista_after(userid,rp):
  return rp[np.isnan(rp[userid])!=True].index.tolist()

def table_after(date1,date2,date3):
  rp=revert_pivot_after(date1,date2,date3)
  users=rp.columns.to_numpy()
  vfunc = np.vectorize(r_lista_after, excluded=["rp"], otypes=[list])
  return pd.DataFrame(data={"userid":users, "seen_foll_month":vfunc(users,rp=rp)})



def revert_pivot_after2(date1,date2,date3):
  d2=pd.to_datetime(date2, format='%d/%m/%Y')
  d3=pd.to_datetime(date3, format='%d/%m/%Y')
  tx_user = ratings.loc[(ratings['date'] <= d3) & (ratings['date'] >= d2)]
  tx_user=tx_user[tx_user.movieid.isin(revert_pivot_v(date2,date3).index)]
  index=pd.pivot_table(tx_user, index='movieid', columns='userid', values='rating')
  return index

def table_after_2(date1,date2,date3):
  rp=revert_pivot_after2(date1,date2,date3)
  users=rp.columns.to_numpy()
  vfunc = np.vectorize(r_lista_after, excluded=["rp"], otypes=[set])
  return pd.DataFrame(data={"userid":users, "seen_foll_month":vfunc(users,rp=rp)})


def is_true2(userid,r):
  u=pd.DataFrame(data={"movies": r.loc[userid,"seen_foll_month"]+r.loc[userid,"recomendas"]})
  return len(u[u.duplicated()])

def label2(r):
  users=r.dropna(subset=['seen_foll_month'])["userid"].to_numpy()

  vfunc = np.vectorize(is_true2, excluded=["r"], otypes=[int])
  r_index=r.set_index('userid')
  num=vfunc(users,r=r_index)
  label=pd.DataFrame(data={"userid":users, "n_success_recommendations":num})
  r=pd.merge(r,label,on='userid',how='left')
  r["n_success_recommendations"].fillna(0,inplace=True)
  return r




def K_recomendacoes_associa(date1,date2,date3,k):
  trans=table_viram(date1,date2)
  dataset=trans["seen"].tolist()
  te = TransactionEncoder()
  te_ary = te.fit(dataset).transform(dataset)
  df = pd.DataFrame(te_ary, columns=te.columns_)
  frequent_itemsets = apriori(df, min_support=0.025, use_colnames=True)
  rules=association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)
  rules=rules.sort_values(by=["confidence","support","lift"],ascending=[False,False,False])
  
  rec_a_rules = viram_same(date1,date2)
  rec_a_rules=rec_a_rules.set_index('userid')
  recs= rec_rules(rules,rec_a_rules,k)
  verif=table_after_2('01/05/2007','01/06/2007','01/07/2007')
  recs2=pd.merge(recs,verif,on='userid',how='left')
                           
  recs2=label2(recs2)
  recs2["seen_foll_month"].fillna(0,inplace=True)
  recs2["recomlen"]=recs2["recomendas"].apply(len)
  recs2["P"]=recs2["n_success_recommendations"]/recs2["recomlen"]
  recs2["P"].fillna(0,inplace=True)
  recs2["n_watched"]=recs2["seen_foll_month"].apply(lambda x: len(x) if x!=0 else 0)
  recs2["R"]=recs2["n_success_recommendations"]/recs2["n_watched"]
  recs2["R"].fillna(0,inplace=True)
  return recs2

recomendas= K_recomendacoes_associa('01/05/2007','01/06/2007','01/07/2007',10)
recomendas.head()

"""<div style='text-align: justify;'>  
And then we studied the performance for the usual measures of the algorithm when giving 3, 5, 10 or 20 rules with a minimum support of 0.025 and minimum confidence of 0.5. In general the performance is not as good as expected, or even as good as the one in the very simple popularity algorithm. But one thing we can notice is that the precision does not decrease as much with a higher number of recommendations (and rules) as in the popularity model, in fact our F1 score is increasing with the number of rules. So in this case if we had a better machine to compute the rules with our large dataset we could give more rules and with lower support that would probably fit more users with more particular itemsets.
<div>
"""

precision=[]
percentage=[]
n=[]
recall=[]
f_score=[]
for i in [3,5,10,20]:
  n.append(i)
  r=K_recomendacoes_associa('01/05/2007','01/06/2007','01/07/2007',i)
  precision.append(str(np.mean(r.query("seen_foll_month != 0")["P"])*100)[0:4]+"%")
  recall.append(str(np.mean(r.query("seen_foll_month != 0")["R"])*100)[0:4]+"%")
  p=np.mean(r.query("seen_foll_month != 0")["P"])*100
  R=np.mean(r.query("seen_foll_month != 0")["R"])*100
  f_score.append(str(2*p*R/(R+p))[0:4]+"%")
  percentage.append(str(r.query("n_success_recommendations > 0")["n_success_recommendations"].count()/r.query("seen_foll_month != 0")["n_success_recommendations"].count()*100)[0:4]+"%")
measures = pd.DataFrame(data={"Precision":precision,"Recall":recall,"F1":f_score,"% of users that watched 1 or more recommendations":percentage, "n of rules":n})
measures = measures.set_index('n of rules')

measures

"""### Collaborative Filtering
<br>
<div style='text-align: justify;'> 
&nbsp;&nbsp;Collaborative Filtering Recommendations are based on the similarity of items and the knowledge that a user will likely consume the same items as other users with similar taste did in the past. To make recommendations we do not need to know any information about the content of the items or the profile of the users because item similarity is based only on the differences and resemblance between the sets of users that liked, watched or ignored those items. <br>
&nbsp;&nbsp;Our recommendations can be either user or item based and this basically means that our measures will be computed either row or column-wise, but in general the pipeline for their generation is the following: <ul>
    <li>According to a chosen measure compute the similarity function between <i>items</i> (similar items are rated in a similar way by the same user) or <i>users</i> (similar users have similar ratings on the same item);</li>
    <li>create our model as the item-item or user-user similarity matrix;</li>
    <li>find the k-nearest users or k-nearest items between the pairs in the matrix;</li>
    <li>select N item-based recommendations, giving the user the items most similar to the ones he has seen, or N user-based recommendations by selecting items that are preferred to by his nearest users.</li>
    </ul><br>
&nbsp;&nbsp;This similarity measures can be any of the ones we already know such as the cosine-similarity, the jaccard index, the pearson correlation coefficient or the mean squared difference similarity.
    
#### Implementation     
<br> <br>
&nbsp;&nbsp;For this model we decided to use a <i>scikit</i> package that is specialized in building and analyzing recommendation systems called <i>surprise</i>. This way it was possible to implement several prediction models ready to use with different similarity measures both user and item based. 
<br> <br>
&nbsp;&nbsp;Before using building any model, we first did some analysis of the number of ratings per movie and per user distributions by plotting their histograms. This can help us understands which are the best thresholds we should use to determine if a movie or user are relevant or not, this is, the number of interactions they should have in order not be discarded from the final dataset we are going to be working with. This step is very important because it provides the possibility of reducing the dimensionality of our data and therefore speeds up our processing and running times. The resulting histograms are available in the interactive images of our collab notebook version. 
<br> <br>
&nbsp;&nbsp;After choosing to discard users and movies with less than a thousand ratings in total, we begin looking at the time period of the data that our systems will have access to. We decided we were going to use any ratings from the beginning of the year 2006 to the end of the first semester of 2007. Following this, we had to use a <i>reader</i> to load our pre-processed dataframe in the correct format for the package to use it. And then we divided the data into training and test sets as usual, using the ratio 75%/25%, respectively and created a list of models from the <i>surprise</i> with different measures and ways of calculating the predictions. <br> <br>
&nbsp;&nbsp;For each of the models <i>(KNNBasic</i> and <i>KNNWithZScore</i> we tried 6 optinios with the possible combinations of similarity measures (<i>cosine</i>,<i>msd</i>,<i>pearson</i>) and for item and user based recommendations. After this we computed the predictions and compared the results based on the Root mean squared error to choose the model with the best performance. As we can see in the following table the first 2 models are the ones with lower error, but are very close together, we decided to work with the user-based <i>KNNWithZScore</i> model that uses person correlation as the similarity measure.
<div>

"""

ratings=pd.read_csv("/content/drive/My Drive/ratings.csv",sep='\t', lineterminator='\r',encoding = "UTF-16 LE")
ratings['date']= pd.to_datetime(ratings['date']) 


ratings.drop([8196077],inplace=True)

ratings2 = ratings.copy()
max_date = datetime(2007, 6, 1) 
ratings2 = ratings2.loc[ratings['date'] <= max_date]

min_movie_ratings = 1000
filter_movies = ratings2['movieid'].value_counts() > min_movie_ratings
filter_movies = filter_movies[filter_movies].index.tolist()

min_user_ratings = 1000
filter_users = ratings2['userid'].value_counts() > min_user_ratings
filter_users = filter_users[filter_users].index.tolist()

ratings_new = ratings2[(ratings2['movieid'].isin(filter_movies)) & (ratings2['userid'].isin(filter_users))]
print('The original data frame shape:\t{}'.format(ratings2.shape[0]))
print('The new data frame shape:\t{}'.format(ratings_new.shape[0]))

from surprise import Dataset
from surprise import Reader
from surprise import KNNBasic,KNNWithMeans,KNNWithZScore
from surprise.model_selection import train_test_split
from surprise import accuracy

min_date = datetime(2006, 1, 1) 


def filter_by_date(min_date,max_date,table):
  return table.loc[(table['date'] >= min_date) & (table['date'] <= max_date)]

ratings_new = filter_by_date(min_date,max_date,ratings_new)
ratings_new.movieid = ratings_new.movieid.astype(int)
n_users = ratings_new.userid.unique().shape[0]
n_items = ratings_new.movieid.unique().shape[0]
n_rows = ratings_new.shape[0]
print("Number of Rows: {}, Number of users: {} , Number of movies: {}".format(n_rows, n_users, n_items))

reader = Reader(rating_scale=(0, 5))
data = Dataset.load_from_df(ratings_new[['userid', 'movieid', 'rating']], reader)

# sample random trainset and testset
# test set is made of 25% of the ratings.
trainset, testset = train_test_split(data, test_size=.25)

algo_list = []

# Algorithm parameters to use 
sim_options1 = {'name': 'cosine',
               'user_based': False  #Item-based cosine similarity
               }
sim_options2 = {'name': 'msd',
               'user_based': False  #Item-based msd similarity
               }
sim_options3 = {'name': 'pearson',
               'user_based': False  #Item-based pearson correlation coeficient similarity
               }
sim_options4 = {'name': 'cosine',
               'user_based': True  #User-based cosine similarity
               }
sim_options5 = {'name': 'msd',
               'user_based': True  #User-based msd similarity
               }
sim_options6 = {'name': 'pearson',
               'user_based': True  #User-based pearson correlation coeficient similarity
               }

algo_list.append(KNNBasic(sim_options=sim_options1))
algo_list.append(KNNBasic(sim_options=sim_options2))
algo_list.append(KNNBasic(sim_options=sim_options3))
algo_list.append(KNNBasic(sim_options=sim_options4))
algo_list.append(KNNBasic(sim_options=sim_options5))
algo_list.append(KNNBasic(sim_options=sim_options6))
algo_list.append(KNNWithMeans(sim_options=sim_options1))
algo_list.append(KNNWithMeans(sim_options=sim_options2))
algo_list.append(KNNWithMeans(sim_options=sim_options3))
algo_list.append(KNNWithMeans(sim_options=sim_options4))
algo_list.append(KNNWithMeans(sim_options=sim_options5))
algo_list.append(KNNWithMeans(sim_options=sim_options6))
algo_list.append(KNNWithZScore(sim_options=sim_options1))
algo_list.append(KNNWithZScore(sim_options=sim_options2))
algo_list.append(KNNWithZScore(sim_options=sim_options3))
algo_list.append(KNNWithZScore(sim_options=sim_options4))
algo_list.append(KNNWithZScore(sim_options=sim_options5))
algo_list.append(KNNWithZScore(sim_options=sim_options6))

input_rows = []
# Iterate over all algorithms
for algorithm in algo_list:
    # Train the algorithm on the trainset, and predict ratings for the testset
    algorithm.fit(trainset)
    predictions = algorithm.test(testset)

    # Then compute RMSE
    result = accuracy.rmse(predictions,verbose=False)
    
    #Get algotithm parameters
    similarity = algorithm.sim_options['name']
    base = algorithm.sim_options['user_based']
    if base:
      base = 'user_based'
    else:
      base = 'item_based'

    #Add algorithm full name and result to input_rows list
    input_rows.append((str(algorithm).split(' ')[0].split('.')[-1]+"/"+similarity+"/"+base,result))

rows_list = []
for row in input_rows:
        dict1 = {}
        # get input row in dictionary format
        # key = col_name
        dict1.update(Algorithm = row[0], RMSE = row[1]) 
        rows_list.append(dict1)

performance_compare = pd.DataFrame(rows_list,columns=['Algorithm','RMSE'])
performance_compare.set_index('Algorithm').sort_values('RMSE')

"""<div style='text-align: justify;'>  
After having the chosen model we made rank predictions to latter compute our recommending system. As in all other models we take advantage of <i>numpy</i> array vectorization to accelerate our calculations with such highly dimensional data. Here we can see the predicted rating score for a given movie and a given user head to head with the real prediction. The algorithm did not have access to the real rating, since this was done later in time, and yet it was able to predict almost exactly how much the user would enjoy the movie. Our model predicted a score of 3.56 for the user 288 and the movie 49294, and the real rating was 4, but users can't make non natural number scores, so this prediction is vary accurate.
<div>
"""

def predict_rank(movieid,userid,model):
  preds = model.predict(userid,movieid) #estimar o ranting que o user daria
  return preds[3] # retorna o rating estimado
def r_lista_3(userid,rp,k,model):
  vfunc2 = np.vectorize(predict_rank, excluded=["userid","movielist","ranklist","model"], otypes=[list])
  v2 = rp[np.isnan(rp[userid])].index.to_numpy()
  table = pd.DataFrame(data={"movieid": v2, "rank": vfunc2(v2, userid=userid,model=model)}) #para cada filme dos 50 populares que o user não viu obtenho uma tabela com o filme e o rating estimado para esse par user-filme
  table = table.sort_values(by="rank", ascending=False) #organizo a tabela por ranting estimado 
  return table["movieid"].to_list()[0:k] #devolvo os k filmes com rating estimado mais alto
def table_3(date1,date2,n,model):
  rp=revert_pivot(date1,date2) 
  users=rp.columns.to_numpy()
  vfunc = np.vectorize(r_lista_3, excluded=["rp","k","model"], otypes=[list])
  return pd.DataFrame(data={"userid":users, "recomRank":vfunc(users,rp=rp,k=n,model=model)}) #obtenho a lista de filmes recomendados para cada user organizada pela estimativa de rating
#recomendas=table_3('01/05/2007','01/06/2007',10)

#recomendas=table_3('01/05/2007','01/06/2007',10)
#verif=table_after('01/05/2007','01/06/2007','01/07/2007')
#revert_pivot_after('01/05/2007','01/06/2007','01/07/2007').head()
#recomendas=pd.merge(recomendas,verif,on='userid',how='left')
#recomendas.head()

def is_true_3(userid,r):
  u=pd.DataFrame(data={"movies": r.loc[userid,"seen_foll_month"]+r.loc[userid,"recomRank"]})
  return len(u[u.duplicated()])
def label_3(r):
  users=r.dropna(subset=['seen_foll_month'])["userid"].to_numpy()

  vfunc = np.vectorize(is_true_3, excluded=["r"], otypes=[int])
  r_index=r.set_index('userid')
  num=vfunc(users,r=r_index)
  label=pd.DataFrame(data={"userid":users, "n_success_recommendations":num})
  r=pd.merge(r,label,on='userid',how='left')
  r["n_success_recommendations"].fillna(0,inplace=True)
  return r
#recomendas=label_3(recomendas)
#recomendas.head()

model = algo_list[-1]
model.fit(trainset)
print(model.predict(288,49294.0))
ratings.query('userid == 288 & movieid == 49294.0')

"""<div style='text-align: justify;'>  
And now we can make our recommendations based on the ratings and the usual approach. In the following table we show a sample for the resulting model with 10 given recommendations for the month of June.
<div>
"""

def K_recomendacoes_ranking(date1,date2,date3,N,algo):
  ratings2 = ratings.copy()
  max_date = datetime(2007, 6, 1) 
  ratings2 = ratings2.loc[ratings['date'] <= max_date]

  min_movie_ratings = 1000
  filter_movies = ratings2['movieid'].value_counts() > min_movie_ratings
  filter_movies = filter_movies[filter_movies].index.tolist()

  min_user_ratings = 1000
  filter_users = ratings2['userid'].value_counts() > min_user_ratings
  filter_users = filter_users[filter_users].index.tolist()

  ratings_new = ratings2[(ratings2['movieid'].isin(filter_movies)) & (ratings2['userid'].isin(filter_users))]

  min_date = datetime(int(date1[6:10]), int(date1[3:5]), int(date1[0:2])) 
  max_date = datetime(int(date2[6:10]), int(date2[3:5]), int(date2[0:2])) 

  def filter_by_date(min_date,max_date,table):
    return table.loc[(table['date'] >= min_date) & (table['date'] <= max_date)]

  ratings_new = filter_by_date(min_date,max_date,ratings_new)
  ratings_new.movieid = ratings_new.movieid.astype(int)
  n_users = ratings_new.userid.unique().shape[0]
  n_items = ratings_new.movieid.unique().shape[0]
  n_rows = ratings_new.shape[0]

  reader = Reader(rating_scale=(0, 5))
  data = Dataset.load_from_df(ratings_new[['userid', 'movieid', 'rating']], reader)
  trainset, testset = train_test_split(data, test_size=.25)
  model = algo
  model.fit(trainset)

  recomendas=table_3(date1,date2,N,model)
  verif=table_after(date1,date2,date3)
  recomendas=pd.merge(recomendas,verif,on='userid',how='left')
  recomendas=label_3(recomendas)
  recomendas["seen_foll_month"].fillna(0,inplace=True)
  recomendas["recomlen"]=recomendas["recomRank"].apply(len)
  recomendas["nºvistas"]=recomendas["seen_foll_month"].apply(lambda x: len(x) if x!=0 else 0)
  recomendas["recall"]=recomendas["n_success_recommendations"]/recomendas["nºvistas"]
  recomendas["recall"].fillna(0,inplace=True)
  return recomendas

#K_recomendacoes_ranking('01/05/2007','01/06/2007','01/07/2007',10,algo_list[-1]).head()

"""<div style='text-align: justify;'>  
Finally, as we did in all other approaches, we compute the precision, recall and f1 score given by the model we built when we recommend 3, 5, 10 or 20 movies. As we can see from the table, the increase of the recall and decrease of the precision with the number of recommendations result in a almost constant f1-score. Knowing this we decided to choose the model with 10 recommendations.
<div>
"""

precision=[]
percentage=[]
n=[]
recall=[]
f_score=[]
for i in [3,5,10,20]:
  n.append(i)
  r=K_recomendacoes_ranking('01/05/2007','01/06/2007','01/07/2007',i,algo_list[-1])
  precision.append(str(np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100)[0:4]+"%")
  recall.append(str(np.mean(r.query("seen_foll_month != 0")["recall"])*100)[0:4]+"%")
  p=np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100
  R=np.mean(r.query("seen_foll_month != 0")["recall"])*100
  f_score.append(str(2*p*R/(R+p))[0:4]+"%")
  percentage.append(str(r.query("n_success_recommendations > 0")["n_success_recommendations"].count()/r.query("seen_foll_month != 0")["n_success_recommendations"].count()*100)[0:4]+"%")
measures = pd.DataFrame(data={"Precisão":precision,"Recall":recall,"F1":f_score,"Percentagem de users que viram 1 das recomendações":percentage, "Nº recomendações":n})
measures = measures.set_index('Nº recomendações')
measures

#@title Recommendation generator  
#@markdown Please select a time-window, keep in mind that a bigger time window means more time computing. The date of evaluation is the final date allowed to see if the user has seen the recommendations or not.
Inicial_Date = '2007-02-01'#@param {type:"date"}
Final_Date = '2007-06-01'#@param {type:"date"}
Date_of_evaluation = '2007-07-01'#@param {type:"date"}
K_Recommendations =  #@param {type:"integer"}
Algo = 'Popularity Recommendations'#@param ["Popularity Recommendations", "KNN With ZScore, pearson, user based","KNN WithZScore, pearson, item_based","KNN With Means, msd, item_based","KNN With Means, cosine, item_based"] {allow-input: true}
'01/06/2007'
d1 = Inicial_Date[8:10]+"/"+Inicial_Date[5:7]+"/"+Inicial_Date[0:4]
d2 = Final_Date[8:10]+"/"+Final_Date[5:7]+"/"+Final_Date[0:4]
d3 = Date_of_evaluation[8:10]+"/"+Date_of_evaluation[5:7]+"/"+Date_of_evaluation[0:4]
i = K_Recommendations
if Algo == 'Popularity Recommendations':
  print('Computing Recomendations based on popularity')
  r = K_recomendacoes_populares(d1,d2,d3,K_Recommendations)
  print("Precision = "+str(np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100)[0:4]+"%")
  print("Recall = "+str(np.mean(r.query("seen_foll_month != 0")["recall"])*100)[0:4]+"%")
  p=np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100
  R=np.mean(r.query("seen_foll_month != 0")["recall"])*100
  print("F1 = "+str(2*p*R/(R+p))[0:4]+"%")
  print("Percentage of clients that actually saw at least one recommendation = "+str(r.query("n_success_recommendations > 0")["n_success_recommendations"].count()/r.query("seen_foll_month != 0")["n_success_recommendations"].count()*100)[0:4]+"%")
  print(r[["userid","recommended_p_movieids"]])

if Algo== "KNN With Means, cosine, item_based":
  algo=algo_list[6]
  print('Computing Recomendations based on cosine similarity item based')
  r = K_recomendacoes_ranking(d1,d2,d3,K_Recommendations,algo)
  print("Precision = "+str(np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100)[0:4]+"%")
  print("Recall = "+str(np.mean(r.query("seen_foll_month != 0")["recall"])*100)[0:4]+"%")
  p=np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100
  R=np.mean(r.query("seen_foll_month != 0")["recall"])*100
  print("F1 = "+str(2*p*R/(R+p))[0:4]+"%")
  print("Percentage of clients that actually saw at least one recommendation = "+str(r.query("n_success_recommendations > 0")["n_success_recommendations"].count()/r.query("seen_foll_month != 0")["n_success_recommendations"].count()*100)[0:4]+"%")
  print(r[["userid","recomRank"]])
  
if Algo== "KNN With ZScore, pearson, user based":
  algo=algo_list[-1]
  print('Computing Recomendations based on pearson coefficient user based')
  r = K_recomendacoes_ranking(d1,d2,d3,K_Recommendations,algo)
  print("Precision = "+str(np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100)[0:4]+"%")
  print("Recall = "+str(np.mean(r.query("seen_foll_month != 0")["recall"])*100)[0:4]+"%")
  p=np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100
  R=np.mean(r.query("seen_foll_month != 0")["recall"])*100
  print("F1 = "+str(2*p*R/(R+p))[0:4]+"%")
  print("Percentage of clients that actually saw at least one recommendation = "+str(r.query("n_success_recommendations > 0")["n_success_recommendations"].count()/r.query("seen_foll_month != 0")["n_success_recommendations"].count()*100)[0:4]+"%")
  print(r[["userid","recomRank"]])

if Algo== "KNN WithZScore, pearson, item_based":
  algo=algo_list[14]
  print('Computing Recomendations based on pearson coefficient item based')
  r = K_recomendacoes_ranking(d1,d2,d3,K_Recommendations,algo)
  print("Precision = "+str(np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100)[0:4]+"%")
  print("Recall = "+str(np.mean(r.query("seen_foll_month != 0")["recall"])*100)[0:4]+"%")
  p=np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100
  R=np.mean(r.query("seen_foll_month != 0")["recall"])*100
  print("F1 = "+str(2*p*R/(R+p))[0:4]+"%")
  print("Percentage of clients that actually saw at least one recommendation = "+str(r.query("n_success_recommendations > 0")["n_success_recommendations"].count()/r.query("seen_foll_month != 0")["n_success_recommendations"].count()*100)[0:4]+"%")
  print(r[["userid","recomRank"]])

if Algo== "KNN With Means, msd, item_based":
  algo=algo_list[7]
  print('Computing Recomendations based on msd similarity item based')
  r = K_recomendacoes_ranking(d1,d2,d3,K_Recommendations,algo)
  print("Precision = "+str(np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100)[0:4]+"%")
  print("Recall = "+str(np.mean(r.query("seen_foll_month != 0")["recall"])*100)[0:4]+"%")
  p=np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/i)*100
  R=np.mean(r.query("seen_foll_month != 0")["recall"])*100
  print("F1 = "+str(2*p*R/(R+p))[0:4]+"%")
  print("Percentage of clients that actually saw at least one recommendation = "+str(r.query("n_success_recommendations > 0")["n_success_recommendations"].count()/r.query("seen_foll_month != 0")["n_success_recommendations"].count()*100)[0:4]+"%")
  print(r[["userid","recomRank"]])
  #@markdown Please press control+enter to obtain recommendations

#@title If you want to save the recomendations in your google drive pelase select the following box {run: "auto"}
Export_to_EXCEL = False#@param {type:"boolean", run: "auto"}
if Export_to_EXCEL:
  r.iloc[:,0:2] .to_excel(r'/content/drive/My Drive/recomendasFilipeBiaTiago.xlsx', index = False)
  print ("Exported to your google drive")

"""## Evaluation

<div style='text-align: justify;'>  
<br> <br>
&nbsp;&nbsp;Now that we have our best models for each approach we have do compare their performances. In order to have a fair comparison we need to test them for the same data and periods of time.
    <br>
&nbsp;&nbsp;We decided to compute the 3 measures: precision, recall and f1-score for each of the models through a sliding window that transverses the period of time between January of 2007 and July of the same year. This dates are according to what we have learned in the exploratory data analysis, because this models only work with large amounts of data and this is the period with larger amounts of activity by the users. 
    <br>
&nbsp;&nbsp;We had to exclude the association rules algorithm from this analysis because it takes too long to run, but since it had very low precisions and recalls in comparison to the other 2 methods we think it was not as necessary. 
     <br>
&nbsp;&nbsp;In the following interactive images we can see the performances of the models throughout time.
<div>
"""

def measures_calculator(r):
  precision = str(np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/r.query("seen_foll_month != 0")["recomlen"])*100)[0:4]
  recall = str(np.mean(r.query("seen_foll_month != 0")["recall"])*100)[0:4]
  p= np.mean(r.query("seen_foll_month != 0")["n_success_recommendations"]/r.query("seen_foll_month != 0")["recomlen"])*100
  R= np.mean(r.query("seen_foll_month != 0")["recall"])*100
  f_score = str(2*p*R/(R+p))[0:4]
  return precision, recall, f_score
#p, r, f = measures_calculator(K_recomendacoes_ranking('01/05/2007', '01/06/2007', '01/07/2007', 3, algo_list[-1]))
#print(p, r, f)

CF_p= [18.0,  16.2, 15.7, 12.2, 12.6, 12.5]
CF_r= [40.5, 38.9, 40.4, 28.9, 36.3, 31.9]
CF_f= [24.9, 22.8, 22.6, 17.2, 18.8, 18.0 ]
P_p= [17.5, 16.4, 17.4, 13.1, 21.2, 16.9]
P_r= [23.9, 27.7, 35.8, 25.7, 42.3, 30.2]
P_f= [20.2, 20.6, 23.4, 17.4, 28.3, 21.7]
data= ['01/02/2007', '01/03/2007', '01/04/2007', '01/05/2007', '01/06/2007', '01/07/2007']

title = 'Precision of recommendations over time'
labels = ['Popularity', 'Colaborative filtering']
colors = ['blue', 'grey']
precision=[P_p, CF_p]

mode_size = [8, 8, 12, 8]
line_size = [2, 2, 4, 2]
fig = go.Figure()

for i in range(len(precision)):
    Y = precision[i]
    fig.add_trace(go.Scatter(x=data, y= Y, mode='markers+lines',
        name=labels[i],
        line=dict(color=colors[i], width=line_size[i]),
        connectgaps=True,
    ))

    # endpoints
    fig.add_trace(go.Scatter(
        x=[data[0], data[-1]],
        y=[Y[0], Y[-1]],
        mode='markers',
        marker=dict(color=colors[i], size=mode_size[i])
    ))

fig.update_layout(title=title,
    xaxis=dict(
        showline=True,
        showgrid=True,
        showticklabels=True,
        linecolor='rgb(204, 204, 204)',
        linewidth=2,
        ticks='outside',
        tickfont=dict(
            family='Arial',
            size=12,
            color='rgb(82, 82, 82)',
        ),
    ),
    yaxis=dict(
        showgrid=True,
        zeroline=True,
        showline=True,
        showticklabels=True,
    ),
    autosize=False,
    margin=dict(
        autoexpand=False,
        l=50,
        r=150,
        t=110,
    ),
    showlegend=False,
    plot_bgcolor='white'
)

annotations = []

# Adding labels
for y_trace, label, color in zip(precision, labels, colors):
    # labeling the left_side of the plot
    annotations.append(dict(xref='paper', x=0.1, y=y_trace[0]+0.05,
                                  xanchor='right', yanchor='middle',
                                  text='{}%'.format(y_trace[0]),
                                  font=dict(family='Arial', color=color,
                                            size=12),
                                  showarrow=False))
    # labeling the right_side of the plot
    annotations.append(dict(xref='paper', x=0.95, y=y_trace[-1],
                                  xanchor='left', yanchor='middle',
                                  text=label + '{}%'.format(y_trace[-1]),
                                  font=dict(family='Arial',
                                            size=12),
                                  showarrow=False))
# Title


fig.update_layout(annotations=annotations)

fig.show()

title = 'Recall of recommendations over time'
labels = ['Popularity', 'Colaborative filtering']
colors = ['blue', 'grey']
precision=[P_r, CF_r]

mode_size = [8, 8, 12, 8]
line_size = [2, 2, 4, 2]
fig = go.Figure()

for i in range(len(precision)):
    Y = precision[i]
    fig.add_trace(go.Scatter(x=data, y= Y, mode='markers+lines',
        name=labels[i],
        line=dict(color=colors[i], width=line_size[i]),
        connectgaps=True,
    ))

    # endpoints
    fig.add_trace(go.Scatter(
        x=[data[0], data[-1]],
        y=[Y[0], Y[-1]],
        mode='markers',
        marker=dict(color=colors[i], size=mode_size[i])
    ))

fig.update_layout(title=title,
    xaxis=dict(
        showline=True,
        showgrid=True,
        showticklabels=True,
        linecolor='rgb(204, 204, 204)',
        linewidth=2,
        ticks='outside',
        tickfont=dict(
            family='Arial',
            size=12,
            color='rgb(82, 82, 82)',
        ),
    ),
    yaxis=dict(
        showgrid=True,
        zeroline=True,
        showline=True,
        showticklabels=True,
    ),
    autosize=False,
    margin=dict(
        autoexpand=False,
        l=50,
        r=150,
        t=110,
    ),
    showlegend=False,
    plot_bgcolor='white'
)

annotations = []

# Adding labels
for y_trace, label, color in zip(precision, labels, colors):
    # labeling the left_side of the plot
    annotations.append(dict(xref='paper', x=0.1, y=y_trace[0]+0.05,
                                  xanchor='right', yanchor='middle',
                                  text='{}%'.format(y_trace[0]),
                                  font=dict(family='Arial', color=color,
                                            size=12),
                                  showarrow=False))
    # labeling the right_side of the plot
    annotations.append(dict(xref='paper', x=0.95, y=y_trace[-1],
                                  xanchor='left', yanchor='middle',
                                  text=label + '{}%'.format(y_trace[-1]),
                                  font=dict(family='Arial',
                                            size=12),
                                  showarrow=False))
# Title


fig.update_layout(annotations=annotations)

fig.show()

title = 'F1 Score of recommendations over time'
labels = ['Popularity', 'Colaborative filtering']
colors = ['blue', 'grey']
precision=[P_f, CF_f]

mode_size = [8, 8, 12, 8]
line_size = [2, 2, 4, 2]
fig = go.Figure()

for i in range(len(precision)):
    Y = precision[i]
    fig.add_trace(go.Scatter(x=data, y= Y, mode='markers+lines',
        name=labels[i],
        line=dict(color=colors[i], width=line_size[i]),
        connectgaps=True,
    ))

    # endpoints
    fig.add_trace(go.Scatter(
        x=[data[0], data[-1]],
        y=[Y[0], Y[-1]],
        mode='markers',
        marker=dict(color=colors[i], size=mode_size[i])
    ))

fig.update_layout(title=title,
    xaxis=dict(
        showline=True,
        showgrid=True,
        showticklabels=True,
        linecolor='rgb(204, 204, 204)',
        linewidth=2,
        ticks='outside',
        tickfont=dict(
            family='Arial',
            size=12,
            color='rgb(82, 82, 82)',
        ),
    ),
    yaxis=dict(
        showgrid=True,
        zeroline=True,
        showline=True,
        showticklabels=True
    ),
    autosize=False,
    margin=dict(
        autoexpand=False,
        l=50,
        r=150,
        t=110,
    ),
    showlegend=False,
    plot_bgcolor='white'
)

annotations = []

# Adding labels
for y_trace, label, color in zip(precision, labels, colors):
    # labeling the left_side of the plot
    annotations.append(dict(xref='paper', x=0.1, y=y_trace[0]-0.1,
                                  xanchor='right', yanchor='middle',
                                  text='{}%'.format(y_trace[0]),
                                  font=dict(family='Arial', color=color,
                                            size=12),
                                  showarrow=False))
    # labeling the right_side of the plot
    annotations.append(dict(xref='paper', x=0.95, y=y_trace[-1],
                                  xanchor='left', yanchor='middle',
                                  text=label + '{}%'.format(y_trace[-1]),
                                  font=dict(family='Arial',
                                            size=12),
                                  showarrow=False))
# Title


fig.update_layout(annotations=annotations)

fig.show()